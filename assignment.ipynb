{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNIBQZig6LPp"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "eec4b482-f998-4f1b-e736-3b9db9970032"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.feature_selection import (SelectKBest, VarianceThreshold,\n",
        "                                       mutual_info_classif)\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import (GridSearchCV, KFold, cross_val_score,\n",
        "                                     learning_curve, train_test_split)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# import data\n",
        "from worclipo.load_data import load_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions to split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to split the dataset into train and test\n",
        "def split_set(X,y,test_size):\n",
        "\n",
        "    if os.path.exists('./TEST_set.csv'):\n",
        "        split_action = print('TEST_set.csv already exists')\n",
        "    else:\n",
        "        split_action = print('TEST_set.csv does not exist, generating new test and training sets')\n",
        "        X_train_csv, X_test_csv, y_train_csv, y_test_csv = train_test_split(X, y, test_size=test_size, random_state=10)\n",
        "\n",
        "        TESTSET = X_test_csv.merge(y_test_csv, left_index=True, right_index=True)\n",
        "        TESTSET.to_csv('TEST_set.csv')\n",
        "\n",
        "        TRAINSET = X_train_csv.merge(y_train_csv, left_index=True, right_index=True)\n",
        "        TRAINSET.to_csv('TRAIN_set.csv')\n",
        "        return split_action"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting up the data to be processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "print(type(data))\n",
        "\n",
        "# change lipoma = 1 and liposarcoma = 0 and encode labels\n",
        "group_names = list(set(data.label))\n",
        "data.loc[data['label'] == 'lipoma', 'label'] = 1\n",
        "data.loc[data['label'] == 'liposarcoma', 'label'] = 0\n",
        "data['label'] = pd.cut(data['label'], bins = 2, labels=group_names)\n",
        "print(data['label'].unique())\n",
        "label_diag = LabelEncoder()\n",
        "data['label'] = label_diag.fit_transform(data['label'])\n",
        "\n",
        "# assign X to measurements and y to outcome (lipoma/sarcoma)\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']\n",
        "test_size = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code that splits the data into test and validation sets if this is not done already\n",
        "split_set(X,y,test_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import\n",
        "TRAIN = pd.read_csv('TRAIN_set.csv', index_col=0)\n",
        "X_train = TRAIN.drop('label', axis=1)\n",
        "y_train = TRAIN['label']\n",
        "\n",
        "TRAIN['label'] = pd.cut(TRAIN['label'], bins = 2, labels=group_names)\n",
        "print(TRAIN['label'].unique())\n",
        "label_diag = LabelEncoder()\n",
        "TRAIN['label'] = label_diag.fit_transform(TRAIN['label'])\n",
        "\n",
        "# split into training and validation set\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=10)\n",
        "\n",
        "print(f'Size before preprocess: ', X_train.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gridsearch for best classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_FEATURES_OPTIONS = [15, 18, 19, 20, 22, 25]\n",
        "\n",
        "# set up pipeline steps\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "scaler = StandardScaler()\n",
        "variance = VarianceThreshold(threshold=0)\n",
        "pca = PCA()\n",
        "\n",
        "# set up classifiers\n",
        "clf1 = RandomForestClassifier(random_state=1)\n",
        "clf2 = SVC(probability=True, random_state=1, max_iter=1000)\n",
        "clf3 = LogisticRegression(random_state=1)\n",
        "clf4 = DecisionTreeClassifier(random_state=1)\n",
        "clf5 = KNeighborsClassifier()\n",
        "clf6 = MultinomialNB()\n",
        "\n",
        "pipe = Pipeline(steps=[('imputer', imputer), ('scaler', scaler), ('variance', variance), ('pca', pca), ('classifier', clf2)])\n",
        "\n",
        "# set up parameters\n",
        "param1 = {}\n",
        "param1['classifier__n_estimators'] = [1,10, 50, 100, 250]\n",
        "param1['classifier__max_depth'] = [5, 10, 20]\n",
        "param1['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]\n",
        "param1['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param1['classifier'] = [clf1]\n",
        "\n",
        "param2 = {}\n",
        "param2['classifier__C'] = [0.1, 1, 10, 100]\n",
        "param2['classifier__class_weight'] = [None]\n",
        "param2['classifier__kernel'] = ['sigmoid']\n",
        "param2['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param2['classifier'] = [clf2]\n",
        "\n",
        "param3 = {}\n",
        "param3['classifier__C'] = [10**-2, 10**-1, 10**0, 10**1, 10**2]\n",
        "param3['classifier__penalty'] = [None, 'l2']\n",
        "param3['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:6}, {0:1,1:7}]\n",
        "param3['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param3['classifier'] = [clf3]\n",
        "\n",
        "param4 = {}\n",
        "param4['classifier__max_depth'] = [5,10,25,None]\n",
        "param4['classifier__min_samples_split'] = [2,5,10]\n",
        "param4['classifier__class_weight'] = [{0:1,1:2}, {0:1,1:3}, {0:1,1:4}, {0:1,1:5}]\n",
        "param4['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param4['classifier'] = [clf4]\n",
        "\n",
        "param5 = {}\n",
        "param5['classifier__n_neighbors'] = [2,5,10,25,50]\n",
        "param5['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param5['classifier'] = [clf5]\n",
        "\n",
        "param6 = {}\n",
        "param6['classifier__alpha'] = [10**0, 10**1, 10**2]\n",
        "param6['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param6['classifier'] = [clf6]\n",
        "\n",
        "param_grid = [param1, param2, param3, param4, param5, param6]\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)\n",
        "print(grid.best_estimator_)\n",
        "print(grid.best_score_)\n",
        "\n",
        "# Test data performance\n",
        "print(\"Test Precision:\",precision_score(grid.predict(X_valid), y_valid))\n",
        "print(\"Test Recall:\",recall_score(grid.predict(X_valid), y_valid))\n",
        "print(\"Test ROC AUC Score:\",roc_auc_score(grid.predict(X_valid), y_valid))\n",
        "\n",
        "best_clf = grid.best_estimator_\n",
        "best_clf.fit(X_train,y_train)\n",
        "pred_rfc = best_clf.predict(X_valid)\n",
        "\n",
        "print('rfc', classification_report(y_valid, pred_rfc))\n",
        "print(confusion_matrix(y_valid, pred_rfc))\n",
        "\n",
        "results = pd.DataFrame(grid.cv_results_)\n",
        "results = results.sort_values(by=['rank_test_score'])\n",
        "results.to_csv('results')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Validation KFold selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trying differen K folds in cross validation per classifier\n",
        "# Used to give us an idea on which Kfold to use\n",
        "\n",
        "# function\n",
        "def perform_cross_validation(clf, k_values, X_train, y_train):\n",
        "    best_k = None\n",
        "    best_score = None\n",
        "    results = []\n",
        "    for k in k_values:\n",
        "        kf = KFold(n_splits=k, shuffle=True, random_state=10)\n",
        "        scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n",
        "        mean_score = np.mean(scores)\n",
        "        results.append((k, scores))\n",
        "        if best_score is None or mean_score > best_score:\n",
        "            best_k = k\n",
        "            best_score = mean_score\n",
        "    return best_k, results\n",
        "\n",
        "classifiers = [\n",
        "    LogisticRegression(),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    KNeighborsClassifier(),\n",
        "    SVC()\n",
        "    # MultinomialNB() is left out because it requires different scaled data (MinMax scaler) to work in this code.\n",
        "]\n",
        "\n",
        "k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "best_k_values = {}\n",
        "results = []\n",
        "\n",
        "for clf in classifiers:\n",
        "    clf_name = type(clf).__name__\n",
        "    best_k, cv_results = perform_cross_validation(clf, k_values, X_train, y_train)\n",
        "    best_k_values[clf_name] = best_k\n",
        "    results.append((clf_name, cv_results))\n",
        "    print(f\"The best k-fold value for {clf_name} is {best_k} with a mean accuracy of {np.max([np.mean(cv_scores) for _, cv_scores in cv_results]):.3f}\")\n",
        "\n",
        "for clf_name, cv_results in results:\n",
        "    fig, ax = plt.subplots()\n",
        "    scores = [np.mean(cv_scores) for _, cv_scores in cv_results]\n",
        "    ax.plot(k_values, scores)\n",
        "    ax.scatter(best_k_values[clf_name], np.max(scores), marker='*', s=200, color='r', zorder=10)\n",
        "    ax.set_xlabel('Number of Folds (k)')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title(f'{clf_name} Cross-Validation Performance')\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize the best estimator from the gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# grid search SVC\n",
        "pipe = Pipeline(steps=[('imputer', imputer), ('scaler', scaler), ('variance', variance), ('pca', pca), ('classifier', SVC(probability=True, random_state=8, max_iter=1000))])\n",
        "\n",
        "param_grid = {}\n",
        "param_grid['classifier__C'] = [0.1, 1, 5, 10, 15, 20]\n",
        "param_grid['classifier__class_weight'] = [None]\n",
        "param_grid['classifier__kernel'] = ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "param_grid['pca__n_components'] = [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
        "\n",
        "gsb = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5, scoring='accuracy').fit(X_train, y_train)\n",
        "best_clf1 = gsb.best_estimator_\n",
        "print(best_clf1)\n",
        "\n",
        "# grid search logistic regression\n",
        "pipe = Pipeline(steps=[('imputer', imputer), ('scaler', scaler), ('variance', variance), ('kbest', SelectKBest(mutual_info_classif)), ('classifier', LogisticRegression())])\n",
        "\n",
        "param_grid = {}\n",
        "param_grid['classifier__C'] = [0.1, 1, 10]\n",
        "param_grid['classifier__class_weight'] = [None]\n",
        "param_grid['classifier__penalty'] = [None, 'l2']\n",
        "param_grid['kbest__k'] = [8, 9, 10, 11, 12]\n",
        "\n",
        "gsb = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5, scoring='recall').fit(X_train, y_train)\n",
        "best_clf2 = gsb.best_estimator_\n",
        "print(best_clf2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ensemble of SVC and logistic regression classifiers\n",
        "eclf1 = VotingClassifier(estimators=[('lr', best_clf1), ('rf', best_clf2)], voting='soft')\n",
        "eclf1 = eclf1.fit(X_train, y_train)\n",
        "pred_rfc = eclf1.predict(X_valid)\n",
        "print('rfc', classification_report(y_valid, pred_rfc))\n",
        "print(confusion_matrix(y_valid, pred_rfc))\n",
        "print(\"Test Precision:\",precision_score(eclf1.predict(X_valid), y_valid))\n",
        "print(\"Test Recall:\",recall_score(eclf1.predict(X_valid), y_valid))\n",
        "print(\"Test ROC AUC Score:\",roc_auc_score(eclf1.predict(X_valid), y_valid))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run classifier on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the test set\n",
        "TEST = pd.read_csv('TEST_set.csv', index_col=0)\n",
        "X_test = TEST.drop('label', axis=1)\n",
        "y_test = TEST['label']\n",
        "\n",
        "TEST['label'] = pd.cut(TEST['label'], bins = 2, labels=group_names)\n",
        "label_diag = LabelEncoder()\n",
        "TEST['label'] = label_diag.fit_transform(TEST['label'])\n",
        "\n",
        "# predict\n",
        "pred_test = eclf1.predict(X_test)\n",
        "\n",
        "# results \n",
        "print('Test classification report \\n', classification_report(y_test, pred_test))\n",
        "print(\"Test Precision:    \",precision_score(eclf1.predict(X_test), y_test))\n",
        "print(\"Test Recall:       \",recall_score(eclf1.predict(X_test), y_test))\n",
        "print(\"Test ROC AUC Score:\",roc_auc_score(eclf1.predict(X_test), y_test))\n",
        " \n",
        "print('confusion matrix \\n', confusion_matrix(y_valid, pred_rfc))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_val = eclf1.predict_proba(X_valid)[:,1]\n",
        "\n",
        "fpr_val, tpr_val, _ = metrics.roc_curve(y_valid, pred_val)\n",
        "auc_val = metrics.roc_auc_score(y_valid, pred_val)\n",
        "\n",
        "pred_test = eclf1.predict_proba(X_test)[:,1]\n",
        "\n",
        "fpr_test, tpr_test, _ = metrics.roc_curve(y_test, pred_test)\n",
        "auc_test = metrics.roc_auc_score(y_test, pred_test)\n",
        "\n",
        "plt.plot(fpr_val,tpr_val,label=\"Validation set, AUC=\"+str(round(auc_val, 5)))\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.title('ROCs of validation set and test set')\n",
        "plt.legend(loc=4)\n",
        "\n",
        "plt.plot(fpr_test, tpr_test,label='Test set, AUC='+str(round(auc_test,5)))\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sizes, train_scores, test_scores = learning_curve(estimator=eclf1, X=X_train, y=y_train,\n",
        "                                                        cv=5, train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "                                                        n_jobs=1)\n",
        "\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
        "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
        "plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
        "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Training Data Size')\n",
        "plt.ylabel('Model accuracy')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Number of features - SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('reduce_dim', 'passthrough'),\n",
        "    ('classify', SVC(kernel='sigmoid', max_iter=1000))\n",
        "])\n",
        "\n",
        "N_FEATURES_OPTIONS = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "C_OPTIONS = [0.1, 1, 10, 25]\n",
        "param_grid = [\n",
        "    {\n",
        "        'reduce_dim': [PCA()],\n",
        "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
        "        'classify__C': C_OPTIONS\n",
        "    },\n",
        "    {\n",
        "        'reduce_dim': [SelectKBest(mutual_info_classif)],\n",
        "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
        "        'classify__C': C_OPTIONS\n",
        "    }\n",
        "]\n",
        "reducer_labels = ['PCA', 'KBest(mutual_info_classif)']\n",
        "\n",
        "grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
        "# scores are in the order of param_grid iteration, which is alphabetical\n",
        "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
        "# select score for best C\n",
        "mean_scores = mean_scores.max(axis=0)\n",
        "mean_scores = pd.DataFrame(\n",
        "    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n",
        ")\n",
        "\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "# plot barplot with results\n",
        "ax = mean_scores.plot.bar()\n",
        "ax.set_title(\"Comparing feature selection techniques - SVM\")\n",
        "ax.set_xlabel(\"Reduced number of features\")\n",
        "ax.set_ylabel(\"Digit classification accuracy\")\n",
        "ax.set_ylim((0, 1))\n",
        "ax.legend(loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Number of features - Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('reduce_dim', 'passthrough'),\n",
        "    ('classify', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "N_FEATURES_OPTIONS = [5, 10, 15, 20, 25, 30, 35, 40]\n",
        "C_OPTIONS = [0.01, 0.1, 1, 10, 25]\n",
        "param_grid = [\n",
        "    {\n",
        "        'reduce_dim': [PCA()],\n",
        "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
        "        'classify__C': C_OPTIONS\n",
        "    },\n",
        "    {\n",
        "        'reduce_dim': [SelectKBest(mutual_info_classif)],\n",
        "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
        "        'classify__C': C_OPTIONS\n",
        "    }\n",
        "]\n",
        "reducer_labels = ['PCA', 'KBest(mutual_info_classif)']\n",
        "\n",
        "grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid, scoring='recall')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
        "# scores are in the order of param_grid iteration, which is alphabetical\n",
        "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
        "# select score for best C\n",
        "mean_scores = mean_scores.max(axis=0)\n",
        "mean_scores = pd.DataFrame(\n",
        "    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n",
        ")\n",
        "\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "# plot barplot with results\n",
        "ax = mean_scores.plot.bar()\n",
        "ax.set_title(\"Comparing feature selection techniques - Logistic Regression\")\n",
        "ax.set_xlabel(\"Reduced number of features\")\n",
        "ax.set_ylabel(\"Digit classification accuracy\")\n",
        "ax.set_ylim((0, 1))\n",
        "ax.legend(loc=\"upper left\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
