{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNIBQZig6LPp"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "eec4b482-f998-4f1b-e736-3b9db9970032"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import *\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from worclipo.load_data import load_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to split the dataset into train and test\n",
        "def split_set(X,y,test_size):\n",
        "\n",
        "    if os.path.exists('./TEST_set.csv'):\n",
        "        split_action = print('TEST_set.csv already exists')\n",
        "    else:\n",
        "        split_action = print('TEST_set.csv does not exist, generating new test and training sets')\n",
        "        X_train_csv, X_test_csv, y_train_csv, y_test_csv = train_test_split(X, y, test_size=test_size, random_state=10)\n",
        "\n",
        "        TESTSET = X_test_csv.merge(y_test_csv, left_index=True, right_index=True)\n",
        "        TESTSET.to_csv('TEST_set.csv')\n",
        "\n",
        "        TRAINSET = X_train_csv.merge(y_train_csv, left_index=True, right_index=True)\n",
        "        TRAINSET.to_csv('TRAIN_set.csv')\n",
        "        return split_action\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# setting up the data to be processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 115\n",
            "The number of columns: 494\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "['liposarcoma', 'lipoma']\n",
            "Categories (2, object): ['liposarcoma' < 'lipoma']\n"
          ]
        }
      ],
      "source": [
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "print(type(data))\n",
        "\n",
        "# change lipoma = 1 and liposarcoma = 0 and encode labels\n",
        "group_names = list(set(data.label))\n",
        "data.loc[data['label'] == 'lipoma', 'label'] = 1\n",
        "data.loc[data['label'] == 'liposarcoma', 'label'] = 0\n",
        "data['label'] = pd.cut(data['label'], bins = 2, labels=group_names)\n",
        "print(data['label'].unique())\n",
        "label_diag = LabelEncoder()\n",
        "data['label'] = label_diag.fit_transform(data['label'])\n",
        "\n",
        "# assign X to measurements and y to outcome (lipoma/sarcoma)\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']\n",
        "test_size = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST_set.csv already exists\n"
          ]
        }
      ],
      "source": [
        "# code that splits the data into test and validation sets if this is not done already\n",
        "split_set(X,y,test_size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## import the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['liposarcoma', 'lipoma']\n",
            "Categories (2, object): ['liposarcoma' < 'lipoma']\n",
            "Size before preprocess:  (56, 493)\n"
          ]
        }
      ],
      "source": [
        "TRAIN = pd.read_csv('TRAIN_set.csv', index_col=0)\n",
        "X_train = TRAIN.drop('label', axis=1)\n",
        "y_train = TRAIN['label']\n",
        "\n",
        "TRAIN['label'] = pd.cut(TRAIN['label'], bins = 2, labels=group_names)\n",
        "print(TRAIN['label'].unique())\n",
        "label_diag = LabelEncoder()\n",
        "TRAIN['label'] = label_diag.fit_transform(TRAIN['label'])\n",
        "\n",
        "# split into training and validation set\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=10)\n",
        "\n",
        "print(f'Size before preprocess: ', X_train.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "90 fits failed out of a total of 4800.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "90 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py\", line 776, in fit\n",
            "    self._count(X, Y)\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py\", line 898, in _count\n",
            "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py\", line 1418, in check_non_negative\n",
            "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.55757576 0.60454545 0.58939394 0.53939394 0.64545455 0.58787879\n",
            " 0.57272727 0.62727273 0.53787879 0.59242424 0.49848485 0.60757576\n",
            " 0.57575758 0.57272727 0.59090909 0.53636364 0.5530303  0.58939394\n",
            " 0.61060606 0.59090909 0.59242424 0.57424242 0.53484848 0.55606061\n",
            " 0.59242424 0.59242424 0.59090909 0.51969697 0.55606061 0.57424242\n",
            " 0.55757576 0.65909091 0.58939394 0.53939394 0.61060606 0.58787879\n",
            " 0.59090909 0.61060606 0.57424242 0.59242424 0.51666667 0.62727273\n",
            " 0.59393939 0.59090909 0.60757576 0.5030303  0.55454545 0.57272727\n",
            " 0.59393939 0.59090909 0.59090909 0.55606061 0.58939394 0.55606061\n",
            " 0.59393939 0.59242424 0.59090909 0.53787879 0.51969697 0.59090909\n",
            " 0.55757576 0.65909091 0.58939394 0.53939394 0.61060606 0.58787879\n",
            " 0.59090909 0.61060606 0.57424242 0.59242424 0.51666667 0.62727273\n",
            " 0.59393939 0.59090909 0.60757576 0.5030303  0.55454545 0.57272727\n",
            " 0.59393939 0.59090909 0.59090909 0.55606061 0.58939394 0.55606061\n",
            " 0.59393939 0.59242424 0.59090909 0.53787879 0.51969697 0.59090909\n",
            " 0.5030303  0.46969697 0.60909091 0.45151515 0.58939394 0.37727273\n",
            " 0.57121212 0.46515152 0.46363636 0.59393939 0.5530303  0.48181818\n",
            " 0.57121212 0.51969697 0.50151515 0.50151515 0.57424242 0.46666667\n",
            " 0.60757576 0.44848485 0.50151515 0.48181818 0.50151515 0.50151515\n",
            " 0.58939394 0.52121212 0.51969697 0.50151515 0.50151515 0.48636364\n",
            " 0.59242424 0.50606061 0.63030303 0.43333333 0.55606061 0.41212121\n",
            " 0.62727273 0.48333333 0.57121212 0.55909091 0.57121212 0.44545455\n",
            " 0.66060606 0.55454545 0.58939394 0.53636364 0.50151515 0.53787879\n",
            " 0.64545455 0.53636364 0.62424242 0.57272727 0.60909091 0.53636364\n",
            " 0.60909091 0.49848485 0.59090909 0.58939394 0.55606061 0.51969697\n",
            " 0.59242424 0.50606061 0.63030303 0.43333333 0.55606061 0.41212121\n",
            " 0.62727273 0.48333333 0.57121212 0.55909091 0.57121212 0.44545455\n",
            " 0.66060606 0.55454545 0.58939394 0.53636364 0.50151515 0.53787879\n",
            " 0.64545455 0.53636364 0.62424242 0.57272727 0.60909091 0.53636364\n",
            " 0.60909091 0.49848485 0.59090909 0.58939394 0.55606061 0.51969697\n",
            " 0.46818182 0.45151515 0.60909091 0.43333333 0.3969697  0.34090909\n",
            " 0.51818182 0.53787879 0.48484848 0.46363636 0.51666667 0.46666667\n",
            " 0.51818182 0.50151515 0.46515152 0.42878788 0.50151515 0.48484848\n",
            " 0.51818182 0.50151515 0.48333333 0.48333333 0.48181818 0.46515152\n",
            " 0.53636364 0.48333333 0.51969697 0.48333333 0.51969697 0.46818182\n",
            " 0.48787879 0.48787879 0.59393939 0.41515152 0.41515152 0.35757576\n",
            " 0.55454545 0.5030303  0.55454545 0.46818182 0.55454545 0.48181818\n",
            " 0.64393939 0.53787879 0.53787879 0.46515152 0.55606061 0.48333333\n",
            " 0.64393939 0.55454545 0.59090909 0.55454545 0.57575758 0.51969697\n",
            " 0.66363636 0.48484848 0.60757576 0.57272727 0.57424242 0.5030303\n",
            " 0.48787879 0.48787879 0.59393939 0.41515152 0.41515152 0.39393939\n",
            " 0.57272727 0.52121212 0.55454545 0.46818182 0.55454545 0.46363636\n",
            " 0.64393939 0.53787879 0.53787879 0.46515152 0.55606061 0.50151515\n",
            " 0.64393939 0.55454545 0.59090909 0.57121212 0.57575758 0.51969697\n",
            " 0.66363636 0.50151515 0.60757576 0.57272727 0.57424242 0.5030303\n",
            " 0.45       0.48787879 0.50151515 0.36060606 0.41515152 0.39545455\n",
            " 0.44545455 0.5030303  0.46818182 0.39090909 0.43030303 0.41363636\n",
            " 0.5        0.48333333 0.53787879 0.4469697  0.42878788 0.46515152\n",
            " 0.5        0.48333333 0.48333333 0.4469697  0.4469697  0.46515152\n",
            " 0.48181818 0.50151515 0.50151515 0.46515152 0.46515152 0.46515152\n",
            " 0.45151515 0.52424242 0.41515152 0.37878788 0.45151515 0.37575758\n",
            " 0.62575758 0.50454545 0.52121212 0.50151515 0.53787879 0.50151515\n",
            " 0.64393939 0.53787879 0.53787879 0.51818182 0.52121212 0.50454545\n",
            " 0.66060606 0.5030303  0.59090909 0.53484848 0.55757576 0.46666667\n",
            " 0.60757576 0.53787879 0.53636364 0.53484848 0.53939394 0.48484848\n",
            " 0.45151515 0.52424242 0.43181818 0.37878788 0.45151515 0.41212121\n",
            " 0.60757576 0.52272727 0.52121212 0.51818182 0.53787879 0.51969697\n",
            " 0.62575758 0.53787879 0.55606061 0.53484848 0.52121212 0.48484848\n",
            " 0.66060606 0.5030303  0.59090909 0.5530303  0.55757576 0.48484848\n",
            " 0.60757576 0.55606061 0.55454545 0.5530303  0.53939394 0.48484848\n",
            " 0.53636364 0.53636364 0.53636364 0.53636364 0.53636364 0.53636364\n",
            " 0.58939394 0.61060606 0.59242424 0.60909091 0.57424242 0.59090909\n",
            " 0.68030303 0.67878788 0.73030303 0.69545455 0.68030303 0.55151515\n",
            " 0.64393939 0.64393939 0.58939394 0.71212121 0.6969697  0.58787879\n",
            " 0.66363636 0.68181818 0.66363636 0.68030303 0.64090909 0.64393939\n",
            " 0.60454545 0.5530303  0.58939394 0.62424242 0.57272727 0.55606061\n",
            " 0.7        0.66363636 0.66363636 0.6969697  0.66060606 0.66060606\n",
            " 0.5530303  0.58787879 0.56969697 0.56969697 0.57121212 0.62424242\n",
            " 0.7        0.66363636 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.5530303  0.55151515 0.55151515 0.55151515 0.56969697 0.62424242\n",
            " 0.7        0.68181818 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.53484848 0.55151515 0.53333333 0.53333333 0.53333333 0.58787879\n",
            " 0.66363636 0.68181818 0.66363636 0.68030303 0.64090909 0.64393939\n",
            " 0.64393939 0.64545455 0.66212121 0.68030303 0.57272727 0.55606061\n",
            " 0.7        0.66363636 0.66363636 0.6969697  0.66060606 0.66060606\n",
            " 0.64242424 0.60909091 0.60909091 0.60757576 0.64545455 0.62727273\n",
            " 0.7        0.66363636 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.62424242 0.57272727 0.60909091 0.60757576 0.64545455 0.62727273\n",
            " 0.7        0.68181818 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.60606061 0.55454545 0.57272727 0.62575758 0.62727273 0.64393939\n",
            " 0.66363636 0.68181818 0.66363636 0.68030303 0.64090909 0.64393939\n",
            " 0.68030303 0.66363636 0.71666667 0.71515152 0.60757576 0.62575758\n",
            " 0.7        0.66363636 0.66363636 0.6969697  0.66060606 0.66060606\n",
            " 0.66363636 0.66212121 0.66212121 0.64242424 0.60757576 0.64242424\n",
            " 0.7        0.66363636 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.66363636 0.66212121 0.66212121 0.62424242 0.60757576 0.66060606\n",
            " 0.7        0.68181818 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.66363636 0.66212121 0.66212121 0.62424242 0.60757576 0.66060606\n",
            " 0.66363636 0.68181818 0.66363636 0.68030303 0.64090909 0.64393939\n",
            " 0.64545455 0.64545455 0.68030303 0.60757576 0.59090909 0.62575758\n",
            " 0.7        0.66363636 0.66363636 0.6969697  0.66060606 0.66060606\n",
            " 0.68030303 0.64545455 0.68030303 0.60757576 0.62575758 0.67727273\n",
            " 0.7        0.66363636 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.68030303 0.64545455 0.68030303 0.60757576 0.62575758 0.67727273\n",
            " 0.7        0.68181818 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.68030303 0.64545455 0.66212121 0.62424242 0.62575758 0.65909091\n",
            " 0.66363636 0.68181818 0.66363636 0.68030303 0.64090909 0.64393939\n",
            " 0.66363636 0.64545455 0.68030303 0.5530303  0.60757576 0.62575758\n",
            " 0.7        0.66363636 0.66363636 0.6969697  0.66060606 0.66060606\n",
            " 0.7        0.62727273 0.69848485 0.53636364 0.60909091 0.65909091\n",
            " 0.7        0.66363636 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.7        0.62727273 0.69848485 0.55454545 0.60909091 0.65909091\n",
            " 0.7        0.68181818 0.68181818 0.68030303 0.66060606 0.67878788\n",
            " 0.7        0.62727273 0.68030303 0.55454545 0.60909091 0.65909091\n",
            " 0.48333333 0.4469697  0.53787879 0.4469697  0.49848485 0.46363636\n",
            " 0.48333333 0.46515152 0.55606061 0.4469697  0.49848485 0.46363636\n",
            " 0.55606061 0.51515152 0.51515152 0.58787879 0.53484848 0.40757576\n",
            " 0.48333333 0.4469697  0.53787879 0.4469697  0.49848485 0.42727273\n",
            " 0.48333333 0.46515152 0.55606061 0.4469697  0.49848485 0.48181818\n",
            " 0.55606061 0.51515152 0.51515152 0.58787879 0.53484848 0.40757576\n",
            " 0.48333333 0.4469697  0.53787879 0.4469697  0.49848485 0.42727273\n",
            " 0.48333333 0.46515152 0.55606061 0.4469697  0.49848485 0.48181818\n",
            " 0.55606061 0.51515152 0.51515152 0.58787879 0.53484848 0.40757576\n",
            " 0.48333333 0.4469697  0.53787879 0.4469697  0.49848485 0.42727273\n",
            " 0.48333333 0.46515152 0.55606061 0.4469697  0.49848485 0.48181818\n",
            " 0.55606061 0.51515152 0.51515152 0.58787879 0.53484848 0.40757576\n",
            " 0.58787879 0.5        0.59090909 0.44545455 0.48333333 0.42727273\n",
            " 0.60606061 0.5        0.59090909 0.44545455 0.48333333 0.44545455\n",
            " 0.60606061 0.5        0.48181818 0.46363636 0.44545455 0.40909091\n",
            " 0.60606061 0.53636364 0.59090909 0.49848485 0.56969697 0.48030303\n",
            " 0.58787879 0.53636364 0.59090909 0.48181818 0.50151515 0.44545455\n",
            " 0.60606061 0.5        0.48181818 0.48181818 0.46363636 0.40909091\n",
            " 0.60606061 0.53636364 0.59090909 0.49848485 0.56969697 0.48030303\n",
            " 0.58787879 0.53636364 0.59090909 0.48181818 0.50151515 0.44545455\n",
            " 0.60606061 0.5        0.48181818 0.48181818 0.46363636 0.40909091\n",
            " 0.60606061 0.53636364 0.59090909 0.49848485 0.56969697 0.48030303\n",
            " 0.58787879 0.53636364 0.59090909 0.48181818 0.50151515 0.44545455\n",
            " 0.60606061 0.5        0.48181818 0.48181818 0.46363636 0.40909091\n",
            " 0.56969697 0.42727273 0.57272727 0.42575758 0.51666667 0.42727273\n",
            " 0.51515152 0.42727273 0.57272727 0.42575758 0.51666667 0.42727273\n",
            " 0.51515152 0.44545455 0.44545455 0.46212121 0.42575758 0.35454545\n",
            " 0.58787879 0.51666667 0.58939394 0.51515152 0.56969697 0.51515152\n",
            " 0.4969697  0.48030303 0.58939394 0.4969697  0.55151515 0.46060606\n",
            " 0.51515152 0.46212121 0.46212121 0.4969697  0.46060606 0.35454545\n",
            " 0.58787879 0.51666667 0.58939394 0.51515152 0.56969697 0.51515152\n",
            " 0.4969697  0.48030303 0.58939394 0.4969697  0.55151515 0.46060606\n",
            " 0.51515152 0.46212121 0.46212121 0.4969697  0.46060606 0.35454545\n",
            " 0.58787879 0.51666667 0.58939394 0.51515152 0.56969697 0.51515152\n",
            " 0.4969697  0.48030303 0.58939394 0.4969697  0.55151515 0.46060606\n",
            " 0.51515152 0.46212121 0.46212121 0.4969697  0.46060606 0.35454545\n",
            " 0.53333333 0.46363636 0.53636364 0.40909091 0.42727273 0.46363636\n",
            " 0.53333333 0.44545455 0.48181818 0.40909091 0.42727273 0.46363636\n",
            " 0.56969697 0.42727273 0.44545455 0.44545455 0.40909091 0.39090909\n",
            " 0.48030303 0.51515152 0.58787879 0.51515152 0.51515152 0.51515152\n",
            " 0.4969697  0.46060606 0.53333333 0.4969697  0.47878788 0.46060606\n",
            " 0.55151515 0.42727273 0.44545455 0.46363636 0.42727273 0.40909091\n",
            " 0.48030303 0.51515152 0.58787879 0.51515152 0.51515152 0.51515152\n",
            " 0.4969697  0.46060606 0.53333333 0.4969697  0.47878788 0.46060606\n",
            " 0.55151515 0.42727273 0.44545455 0.46363636 0.42727273 0.40909091\n",
            " 0.48030303 0.51515152 0.58787879 0.51515152 0.51515152 0.51515152\n",
            " 0.4969697  0.46060606 0.53333333 0.4969697  0.47878788 0.46060606\n",
            " 0.55151515 0.42727273 0.44545455 0.46363636 0.42727273 0.40909091\n",
            " 0.60909091 0.60909091 0.60757576 0.60757576 0.57272727 0.58939394\n",
            " 0.57424242 0.57575758 0.57272727 0.57272727 0.53787879 0.55454545\n",
            " 0.59090909 0.60909091 0.59242424 0.59090909 0.59242424 0.57424242\n",
            " 0.59090909 0.62575758 0.62575758 0.62575758 0.58939394 0.62575758\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'classifier': SVC(C=10, kernel='sigmoid', max_iter=1000, probability=True, random_state=1), 'classifier__C': 10, 'classifier__class_weight': None, 'classifier__kernel': 'sigmoid', 'pca__n_components': 19}\n",
            "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
            "                ('variance', VarianceThreshold(threshold=0)),\n",
            "                ('pca', PCA(n_components=19)),\n",
            "                ('classifier',\n",
            "                 SVC(C=10, kernel='sigmoid', max_iter=1000, probability=True,\n",
            "                     random_state=1))])\n",
            "0.7303030303030302\n",
            "Test Precision: 0.6153846153846154\n",
            "Test Recall: 0.8888888888888888\n",
            "Test ROC AUC Score: 0.7777777777777778\n",
            "rfc               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.91      0.77        11\n",
            "           1       0.89      0.62      0.73        13\n",
            "\n",
            "    accuracy                           0.75        24\n",
            "   macro avg       0.78      0.76      0.75        24\n",
            "weighted avg       0.79      0.75      0.75        24\n",
            "\n",
            "[[10  1]\n",
            " [ 5  8]]\n",
            "GRID 2\n",
            "rfc2               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.18      0.29        11\n",
            "           1       0.57      0.92      0.71        13\n",
            "\n",
            "    accuracy                           0.58        24\n",
            "   macro avg       0.62      0.55      0.50        24\n",
            "weighted avg       0.62      0.58      0.51        24\n",
            "\n",
            "[[ 2  9]\n",
            " [ 1 12]]\n",
            "ENSEMBLE\n",
            "rfc3               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.45      0.56        11\n",
            "           1       0.65      0.85      0.73        13\n",
            "\n",
            "    accuracy                           0.67        24\n",
            "   macro avg       0.68      0.65      0.64        24\n",
            "weighted avg       0.68      0.67      0.65        24\n",
            "\n",
            "[[ 5  6]\n",
            " [ 2 11]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
            "90 fits failed out of a total of 4800.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "90 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py\", line 776, in fit\n",
            "    self._count(X, Y)\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\naive_bayes.py\", line 898, in _count\n",
            "    check_non_negative(X, \"MultinomialNB (input X)\")\n",
            "  File \"C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\validation.py\", line 1418, in check_non_negative\n",
            "    raise ValueError(\"Negative values in data passed to %s\" % whom)\n",
            "ValueError: Negative values in data passed to MultinomialNB (input X)\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "C:\\Users\\ethie\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.46       0.60666667 0.5        0.54666667 0.58666667 0.58\n",
            " 0.34666667 0.42666667 0.28       0.43333333 0.41333333 0.35333333\n",
            " 0.31333333 0.34666667 0.34666667 0.34666667 0.34666667 0.46\n",
            " 0.38666667 0.38666667 0.35333333 0.42666667 0.34666667 0.38666667\n",
            " 0.38666667 0.35333333 0.34666667 0.31333333 0.31333333 0.34666667\n",
            " 0.46       0.60666667 0.5        0.54666667 0.58666667 0.58\n",
            " 0.34666667 0.35333333 0.28       0.39333333 0.41333333 0.36\n",
            " 0.31333333 0.34666667 0.34666667 0.31333333 0.34666667 0.38666667\n",
            " 0.35333333 0.38666667 0.31333333 0.42666667 0.34666667 0.34666667\n",
            " 0.35333333 0.39333333 0.34666667 0.31333333 0.27333333 0.38\n",
            " 0.46       0.60666667 0.5        0.54666667 0.58666667 0.58\n",
            " 0.34666667 0.35333333 0.28       0.39333333 0.41333333 0.36\n",
            " 0.31333333 0.34666667 0.34666667 0.31333333 0.34666667 0.38666667\n",
            " 0.35333333 0.38666667 0.31333333 0.42666667 0.34666667 0.34666667\n",
            " 0.35333333 0.39333333 0.34666667 0.31333333 0.27333333 0.38\n",
            " 0.5        0.32       0.68666667 0.4        0.68666667 0.46666667\n",
            " 0.53333333 0.42       0.38       0.50666667 0.5        0.46\n",
            " 0.60666667 0.53333333 0.60666667 0.57333333 0.65333333 0.5\n",
            " 0.60666667 0.57333333 0.64666667 0.57333333 0.65333333 0.57333333\n",
            " 0.60666667 0.61333333 0.64666667 0.57333333 0.61333333 0.58\n",
            " 0.50666667 0.36       0.58       0.32       0.58666667 0.5\n",
            " 0.42666667 0.30666667 0.42       0.39333333 0.34       0.34666667\n",
            " 0.52666667 0.45333333 0.48666667 0.53333333 0.34       0.34666667\n",
            " 0.53333333 0.41333333 0.56666667 0.54       0.54       0.46\n",
            " 0.53333333 0.37333333 0.57333333 0.53333333 0.5        0.46\n",
            " 0.50666667 0.36       0.58       0.32       0.58666667 0.5\n",
            " 0.42666667 0.30666667 0.42       0.39333333 0.34       0.34666667\n",
            " 0.52666667 0.45333333 0.48666667 0.53333333 0.34       0.34666667\n",
            " 0.53333333 0.41333333 0.56666667 0.54       0.54       0.46\n",
            " 0.53333333 0.37333333 0.57333333 0.53333333 0.5        0.46\n",
            " 0.58       0.36       0.76666667 0.48       0.43333333 0.54666667\n",
            " 0.53333333 0.58666667 0.58       0.56666667 0.61333333 0.58\n",
            " 0.64666667 0.72666667 0.64666667 0.64666667 0.72666667 0.65333333\n",
            " 0.72       0.72666667 0.72666667 0.68666667 0.72666667 0.68666667\n",
            " 0.72       0.68666667 0.76666667 0.68666667 0.72666667 0.62\n",
            " 0.47333333 0.36       0.54       0.4        0.43333333 0.5\n",
            " 0.42       0.35333333 0.46666667 0.38666667 0.46       0.38666667\n",
            " 0.56666667 0.5        0.56666667 0.52666667 0.57333333 0.46\n",
            " 0.60666667 0.56666667 0.56666667 0.57333333 0.58       0.54\n",
            " 0.57333333 0.45333333 0.64666667 0.57333333 0.58       0.58\n",
            " 0.47333333 0.36       0.54       0.4        0.43333333 0.5\n",
            " 0.42       0.35333333 0.46666667 0.38666667 0.46       0.34666667\n",
            " 0.56666667 0.5        0.56666667 0.52666667 0.57333333 0.46\n",
            " 0.60666667 0.56666667 0.56666667 0.57333333 0.58       0.54\n",
            " 0.57333333 0.45333333 0.64666667 0.57333333 0.58       0.58\n",
            " 0.7        0.4        0.76666667 0.52       0.55333333 0.66666667\n",
            " 0.60666667 0.66       0.66       0.60666667 0.69333333 0.62\n",
            " 0.8        0.80666667 0.80666667 0.72666667 0.72666667 0.72666667\n",
            " 0.84       0.80666667 0.72666667 0.72666667 0.72666667 0.72666667\n",
            " 0.8        0.80666667 0.76666667 0.72666667 0.72666667 0.72666667\n",
            " 0.43333333 0.4        0.34666667 0.4        0.55333333 0.54\n",
            " 0.5        0.47333333 0.46666667 0.34666667 0.54       0.46666667\n",
            " 0.60666667 0.57333333 0.57333333 0.60666667 0.54       0.58\n",
            " 0.6        0.5        0.64666667 0.56666667 0.66       0.54\n",
            " 0.56666667 0.53333333 0.60666667 0.56666667 0.62       0.58\n",
            " 0.43333333 0.4        0.38       0.4        0.55333333 0.54\n",
            " 0.46       0.47333333 0.46666667 0.34666667 0.5        0.46666667\n",
            " 0.56666667 0.57333333 0.57333333 0.60666667 0.54       0.5\n",
            " 0.6        0.5        0.60666667 0.56666667 0.66       0.54\n",
            " 0.56666667 0.53333333 0.60666667 0.56666667 0.58       0.54\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.46666667 0.54666667 0.46666667 0.46666667 0.46666667 0.46666667\n",
            " 0.66       0.66       0.76666667 0.77333333 0.66       0.5\n",
            " 0.70666667 0.7        0.62666667 0.84       0.78       0.61333333\n",
            " 0.51333333 0.54666667 0.54       0.5        0.60666667 0.60666667\n",
            " 0.52666667 0.45333333 0.45333333 0.45333333 0.42       0.42\n",
            " 0.55333333 0.58       0.54       0.61333333 0.61333333 0.57333333\n",
            " 0.88       0.88       0.88       0.88       0.84666667 0.88\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.88       0.88       0.88       0.88       0.88       0.88\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.88       0.88       0.88       0.88       0.88       0.88\n",
            " 0.51333333 0.54666667 0.54       0.5        0.60666667 0.60666667\n",
            " 0.57333333 0.54       0.57333333 0.61333333 0.5        0.46\n",
            " 0.55333333 0.58       0.54       0.61333333 0.61333333 0.57333333\n",
            " 0.76666667 0.69333333 0.69333333 0.68666667 0.73333333 0.65333333\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.76666667 0.69333333 0.69333333 0.68666667 0.73333333 0.65333333\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.76666667 0.69333333 0.69333333 0.72666667 0.73333333 0.68666667\n",
            " 0.51333333 0.54666667 0.54       0.5        0.60666667 0.60666667\n",
            " 0.58666667 0.54666667 0.58       0.61333333 0.46       0.5\n",
            " 0.55333333 0.58       0.54       0.61333333 0.61333333 0.57333333\n",
            " 0.62666667 0.58       0.62       0.61333333 0.54       0.57333333\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.62666667 0.58       0.62       0.61333333 0.54       0.61333333\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.62666667 0.58       0.62       0.61333333 0.54       0.61333333\n",
            " 0.51333333 0.54666667 0.54       0.5        0.60666667 0.60666667\n",
            " 0.51333333 0.54666667 0.54       0.50666667 0.46666667 0.5\n",
            " 0.55333333 0.58       0.54       0.61333333 0.61333333 0.57333333\n",
            " 0.58666667 0.54666667 0.58       0.58       0.54       0.60666667\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.58666667 0.54666667 0.58       0.58       0.54       0.60666667\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.58666667 0.54666667 0.58       0.61333333 0.54       0.60666667\n",
            " 0.51333333 0.54666667 0.54       0.5        0.60666667 0.60666667\n",
            " 0.51333333 0.58666667 0.58       0.42666667 0.46666667 0.5\n",
            " 0.55333333 0.58       0.54       0.61333333 0.61333333 0.57333333\n",
            " 0.55333333 0.54666667 0.58       0.42666667 0.50666667 0.56666667\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.55333333 0.54666667 0.58       0.42666667 0.50666667 0.56666667\n",
            " 0.55333333 0.58       0.54       0.54       0.61333333 0.61333333\n",
            " 0.55333333 0.54666667 0.58       0.42666667 0.50666667 0.56666667\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.43333333 0.56666667 0.60666667 0.56666667 0.49333333 0.53333333\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.43333333 0.56666667 0.60666667 0.56666667 0.49333333 0.53333333\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.43333333 0.56666667 0.60666667 0.56666667 0.49333333 0.53333333\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.31333333 0.34666667 0.42666667 0.42666667 0.53333333 0.5\n",
            " 0.43333333 0.56666667 0.60666667 0.56666667 0.49333333 0.53333333\n",
            " 0.6        0.57333333 0.53333333 0.49333333 0.46       0.49333333\n",
            " 0.56       0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.64       0.65333333 0.57333333 0.53333333 0.49333333 0.49333333\n",
            " 0.53333333 0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.49333333 0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.64       0.65333333 0.57333333 0.53333333 0.49333333 0.49333333\n",
            " 0.53333333 0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.49333333 0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.64       0.65333333 0.57333333 0.53333333 0.49333333 0.49333333\n",
            " 0.53333333 0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.49333333 0.57333333 0.53333333 0.49333333 0.46       0.53333333\n",
            " 0.64       0.65333333 0.57333333 0.53333333 0.49333333 0.49333333\n",
            " 0.56       0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.56       0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.56666667 0.56666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.56666667 0.56666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.56666667 0.56666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.52666667\n",
            " 0.6        0.48666667 0.60666667 0.56666667 0.60666667 0.60666667\n",
            " 0.6        0.48666667 0.60666667 0.56666667 0.60666667 0.60666667\n",
            " 0.68       0.56666667 0.60666667 0.60666667 0.60666667 0.60666667\n",
            " 0.46       0.48666667 0.56666667 0.56666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.60666667\n",
            " 0.46       0.48666667 0.56666667 0.56666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.60666667\n",
            " 0.46       0.48666667 0.56666667 0.56666667 0.60666667 0.52666667\n",
            " 0.49333333 0.48666667 0.60666667 0.56666667 0.60666667 0.52666667\n",
            " 0.64       0.56666667 0.60666667 0.60666667 0.60666667 0.60666667\n",
            " 0.30666667 0.30666667 0.34       0.34       0.30666667 0.34\n",
            " 0.42       0.42       0.45333333 0.45333333 0.42       0.45333333\n",
            " 0.3        0.37333333 0.37333333 0.33333333 0.34       0.37333333\n",
            " 0.2        0.27333333 0.27333333 0.27333333 0.19333333 0.27333333\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "N_FEATURES_OPTIONS = [15, 18, 19, 20, 22, 25]\n",
        "\n",
        "# set up pipeline steps\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "scaler = StandardScaler()\n",
        "variance = VarianceThreshold(threshold=0)\n",
        "pca = PCA()\n",
        "\n",
        "# set up classifiers\n",
        "clf1 = RandomForestClassifier(random_state=1)\n",
        "clf2 = SVC(probability=True, random_state=1, max_iter=1000)\n",
        "clf3 = LogisticRegression(random_state=1)\n",
        "clf4 = DecisionTreeClassifier(random_state=1)\n",
        "clf5 = KNeighborsClassifier()\n",
        "clf6 = MultinomialNB()\n",
        "\n",
        "pipe = Pipeline(steps=[('imputer', imputer), ('scaler', scaler), ('variance', variance), ('pca', pca), ('classifier', clf2)])\n",
        "\n",
        "# set up parameters\n",
        "param1 = {}\n",
        "param1['classifier__n_estimators'] = [1,10, 50, 100, 250]\n",
        "param1['classifier__max_depth'] = [5, 10, 20]\n",
        "param1['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:10}, {0:1,1:25}]\n",
        "param1['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param1['classifier'] = [clf1]\n",
        "\n",
        "param2 = {}\n",
        "param2['classifier__C'] = [0.1, 1, 10, 100]\n",
        "param2['classifier__class_weight'] = [None]\n",
        "param2['classifier__kernel'] = ['sigmoid']\n",
        "param2['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param2['classifier'] = [clf2]\n",
        "\n",
        "param3 = {}\n",
        "param3['classifier__C'] = [10**-2, 10**-1, 10**0, 10**1, 10**2]\n",
        "param3['classifier__penalty'] = [None, 'l2']\n",
        "param3['classifier__class_weight'] = [None, {0:1,1:5}, {0:1,1:6}, {0:1,1:7}]\n",
        "param3['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param3['classifier'] = [clf3]\n",
        "\n",
        "param4 = {}\n",
        "param4['classifier__max_depth'] = [5,10,25,None]\n",
        "param4['classifier__min_samples_split'] = [2,5,10]\n",
        "param4['classifier__class_weight'] = [{0:1,1:2}, {0:1,1:3}, {0:1,1:4}, {0:1,1:5}]\n",
        "param4['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param4['classifier'] = [clf4]\n",
        "\n",
        "param5 = {}\n",
        "param5['classifier__n_neighbors'] = [2,5,10,25,50]\n",
        "param5['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param5['classifier'] = [clf5]\n",
        "\n",
        "param6 = {}\n",
        "param6['classifier__alpha'] = [10**0, 10**1, 10**2]\n",
        "param6['pca__n_components'] = N_FEATURES_OPTIONS\n",
        "param6['classifier'] = [clf6]\n",
        "\n",
        "param_grid = [param1, param2, param3, param4, param5, param6]\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)\n",
        "print(grid.best_estimator_)\n",
        "print(grid.best_score_)\n",
        "\n",
        "# Test data performance\n",
        "print(\"Test Precision:\",precision_score(grid.predict(X_valid), y_valid))\n",
        "print(\"Test Recall:\",recall_score(grid.predict(X_valid), y_valid))\n",
        "print(\"Test ROC AUC Score:\",roc_auc_score(grid.predict(X_valid), y_valid))\n",
        "\n",
        "best_clf = grid.best_estimator_\n",
        "best_clf.fit(X_train,y_train)\n",
        "pred_rfc = best_clf.predict(X_valid)\n",
        "\n",
        "print('rfc', classification_report(y_valid, pred_rfc))\n",
        "print(confusion_matrix(y_valid, pred_rfc))\n",
        "\n",
        "results = pd.DataFrame(grid.cv_results_)\n",
        "results = results.sort_values(by=['rank_test_score'])\n",
        "results.to_csv('results')\n",
        "\n",
        "print('GRID 2')\n",
        "grid2 = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5, scoring='recall')\n",
        "grid2.fit(X_train, y_train)\n",
        "best_clf_2 = grid2.best_estimator_\n",
        "best_clf_2.fit(X_train, y_train)\n",
        "pred_rfc_2 = best_clf_2.predict(X_valid)\n",
        "print('rfc2', classification_report(y_valid, pred_rfc_2))\n",
        "print(confusion_matrix(y_valid, pred_rfc_2))\n",
        "\n",
        "print('ENSEMBLE')\n",
        "vc = VotingClassifier([('clf1', best_clf), ('clf2', best_clf_2)], voting='soft')\n",
        "best_clf_3 = vc.fit(X_train, y_train)\n",
        "pred_rfc_3 = best_clf_3.predict(X_valid)\n",
        "print('rfc3', classification_report(y_valid, pred_rfc_3))\n",
        "print(confusion_matrix(y_valid, pred_rfc_3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
